{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c973a42e-235f-4549-9934-82d0356fe06d",
   "metadata": {},
   "source": [
    "# Exercise 3\n",
    "\n",
    "Exercise 3 includes four problems that you need to solve with programming, and by providing answers to questions. For each problem you need to modify the notebook by adding your own solutions. Remember to save and commit your changes locally, and push your changes to GitHub after each major change! Regular commits will help you to keep track of your changes (and revert them if needed). Pushing your work to GitHub will ensure that you don't lose any work in case your computer crashes (can happen!).\n",
    "\n",
    "**Due date**\n",
    "\n",
    "This exercise should be returned to your personal Github repository within **two weeks** after the exercise was released (by **Friday 3.12 23:59**). Please notice that finishing the programming exercises can take significant amount of time (especially if you don't have yet much programming experience). Hence, **it is recommended that you start immediately working on them**.\n",
    "\n",
    "**Start your exercise in CSC Notebooks**\n",
    "\n",
    "Before you can start programming, you need to launch the CSC Notebook instance and clone your Exercise repository there using Git. If you need help with this, [read the documentation on the course site](https://spatial-analytics.readthedocs.io/en/latest/lessons/L1/git-basics.html).\n",
    "\n",
    "**Hints**\n",
    "\n",
    "If there are general questions arising from this exercise, we will add hints to the course website under [Exercise 3 description](https://spatial-analytics.readthedocs.io/en/latest/lessons/L3/exercise-3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87e1091-08cb-4649-9155-562649c017f1",
   "metadata": {},
   "source": [
    "## Input data\n",
    "\n",
    "In this exercise, we use data from an area in Northern Sweden representing **copper content of rock samples**. \n",
    "\n",
    "- The data is provided to you as a GeoJSON file which is located in `data/copper_data.geojson`. \n",
    "- In the data, you see three attributes `F1`, `F2` and `F3`. First two are horizontal coordinates of sample point in Swedish rectangular coordinate system. The third field `F3` is the relative copper content in soil as mg/kg (ppm). \n",
    "- The data has been obtained from the [Geological Survey of Sweden (SGU)](https://www.sgu.se/en/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab942734-01aa-4a90-9955-264b7a3d5dd5",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "The following functions are provided for you to make things easier. **You need to execute/run this cell before you can use them!**\n",
    "Scroll down the notebook to find the Problems where you should use these functions for different tasks.\n",
    "Hint: You might want to study these functions carefully, and especially the documentation provided for them (as docstrings) to understand how to use them.\n",
    "\n",
    "There are altogether 6 functions below from which you will be using 5 in this exercise:\n",
    "\n",
    "1. `create_hexagon_grid()`\n",
    "2. `interpolate_idw()`\n",
    "3. `get_semivariogram_model()`\n",
    "4. `interpolate_ordinary_kriging()`\n",
    "5. `calculate_RMSE()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62b4154-5693-4998-85c8-4ab598e566a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hexagon_grid(gdf, gridsize=60):\n",
    "    \"\"\"\n",
    "    Generates a Hexagon-grid GeoDataFrame having the same extent as the input gdf. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame with points\n",
    "        \n",
    "        A GeoDataFrame containing points (note: other geometry types won't work)\n",
    "        \n",
    "    gridsize: int \n",
    "    \n",
    "        Number of hexagons that should be created per side. Default 60.\n",
    "    \n",
    "    \"\"\"\n",
    "    import geopandas as gpd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from shapely.geometry import Polygon\n",
    "    import numpy as np\n",
    "\n",
    "    # Get coordinates\n",
    "    x = gdf.geometry.x.values\n",
    "    y = gdf.geometry.y.values\n",
    "\n",
    "    # Create the hexagons\n",
    "    plt.axis('off')\n",
    "    collection = plt.hexbin(x,y, gridsize=gridsize, \n",
    "                           alpha=0, linewidths=0)\n",
    "    \n",
    "    # Parse the hexagon geometries into Shapely Polygons\n",
    "    hex_polys = collection.get_paths()[0].vertices\n",
    "    hex_array = []\n",
    "    for xs, ys in collection.get_offsets():\n",
    "        hex_x = np.add(hex_polys[:,0],  xs)\n",
    "        hex_y = np.add(hex_polys[:,1],  ys)\n",
    "        hex_array.append(Polygon(np.vstack([hex_x, hex_y]).T))\n",
    "        \n",
    "    return gpd.GeoDataFrame({'geometry': hex_array}, crs=gdf.crs)\n",
    "\n",
    "\n",
    "def interpolate_idw(known_gdf, \n",
    "                    unknown_gdf, \n",
    "                    value_column, \n",
    "                    target_column=\"predicted_idw\",\n",
    "                    n_neighbors=-1,\n",
    "                    power=2\n",
    "                   ):\n",
    "    \"\"\"\n",
    "    A helper function to do IDW interpolation for locations defined in 'unknown_points_gdf'.\n",
    "    The values in column `value_column' of the 'known_points_gdf' are used as the basis for doing the interpolation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    known_points_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the known values at given points (i) that are used as the basis for the interpolation.\n",
    "        \n",
    "    unknown_points_gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the unknown locations (x). The geometries can be either points or polygons. \n",
    "        The values for these locations will be predicted using the Inverse Distance Weighting interpolation method.\n",
    "        In case Polygon geometries are passed as input, the centroid of the Polygon will be used as the precise location \n",
    "        for the prediction.\n",
    "    \n",
    "    value_column : str\n",
    "        \n",
    "        The value column in the `known_points_gdf' containing the known values (ùë§ùëñ) used for prediction.\n",
    "        \n",
    "    target_column : str\n",
    "        \n",
    "        The column where the predicted value (ùëßÃÇ) will be stored.\n",
    "        \n",
    "    n_neighbors : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, all neighbors are used in the prediction (i.e. value: -1)\n",
    "        \n",
    "    power : int\n",
    "    \n",
    "        The power (ùõΩ) that defines the distance decay function (by default: 2). Higher power value emphasize the influence of the \n",
    "        points nearest to the unknown point. A smaller power value gives more equal influence of also more distant points.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    from pyinterpolate.idw import inverse_distance_weighting\n",
    "    \n",
    "    # Parse the pointset for the known locations (training dataset)\n",
    "    known_locations = get_pointset_for_interpolation(known_gdf, value_column)\n",
    "\n",
    "    # Create empty column for interpolated values\n",
    "    unknown_gdf[target_column] = None\n",
    "\n",
    "    # Interpolate the values with the test dataset\n",
    "    for row in unknown_gdf.itertuples():\n",
    "        index = row.Index\n",
    "        point = np.array([row.geometry.centroid.x, row.geometry.centroid.y])\n",
    "        # Do the prediction using IDW\n",
    "        prediction = inverse_distance_weighting(known_locations, \n",
    "                                                point, \n",
    "                                                number_of_neighbours=n_neighbors, \n",
    "                                                power=power)\n",
    "        # Assign the value to the result\n",
    "        unknown_gdf.loc[index, target_column] = round(prediction, 2)\n",
    "        \n",
    "    return unknown_gdf\n",
    "\n",
    "\n",
    "def get_semivariogram_model(gdf, \n",
    "                            value_column, \n",
    "                            search_radius, \n",
    "                            max_range,\n",
    "                            weighted,\n",
    "                            n_ranges):\n",
    "    \n",
    "    \"\"\"\n",
    "    A helper function to find optimal semivariogram model for given points in 'gdf'.\n",
    "    The values in column `value_column' of the 'gdf' are used as the empirical basis for finding the semivariogram model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the known values at given points (i) that are used for calculating the semivariance.\n",
    "        \n",
    "    value_column : str\n",
    "        \n",
    "        The value column in the `gdf' containing the known values.\n",
    "        \n",
    "    search_radius : int\n",
    "        \n",
    "        The column where the predicted value (ùëßÃÇ) will be stored.\n",
    "        \n",
    "    max_range : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, all neighbors are used in the prediction (i.e. value: -1)\n",
    "        \n",
    "    weighted : bool\n",
    "    \n",
    "        Whether or not to apply weighting and provide more influence on the closer points.\n",
    "    \n",
    "    n_ranges : int\n",
    "    \n",
    "        The model optimization algorithm divides the study extent into n number of ranges \n",
    "        and tests the theoretical model output against the experimental variogram for each range.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    from pyinterpolate.semivariance import calculate_semivariance, TheoreticalSemivariogram\n",
    "    from pyinterpolate.kriging import Krige\n",
    "    \n",
    "    # Extract x, y and z values from the gdf\n",
    "    known_locations = get_pointset_for_interpolation(gdf, value_column)\n",
    "    \n",
    "    # Empirical semivariogram (based on the known values at given locations)\n",
    "    emp_semivar = calculate_semivariance(data=known_locations, step_size=search_radius, max_range=max_range)\n",
    "\n",
    "    # Theoretical semivariogram\n",
    "    semivar = TheoreticalSemivariogram(points_array=known_locations, empirical_semivariance=emp_semivar)\n",
    "\n",
    "    # Find the optimal model\n",
    "    semivar.find_optimal_model(weighted=weighted, number_of_ranges=n_ranges)\n",
    "    semivar.show_semivariogram()\n",
    "\n",
    "    # Specify the Kriging model\n",
    "    return Krige(semivariogram_model=semivar, known_points=known_locations)\n",
    "    \n",
    "\n",
    "\n",
    "def interpolate_ordinary_kriging(gdf, model, target_col, error_col, n_neighbors=30):\n",
    "    \"\"\"\n",
    "    A helper function to do Ordinary Kriging interpolation for locations defined in 'gdf' \n",
    "    based on given semivariogram 'model'.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf : gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame containing the unknown locations. The geometries can be either points or polygons. \n",
    "        The values for these locations will be predicted using the Ordinary Kriging interpolation method.\n",
    "        In case Polygon geometries are passed as input, the centroid of the Polygon will be used as the precise location \n",
    "        for the prediction.\n",
    "        \n",
    "    model : pyinterpolate.kriging.Krige\n",
    "    \n",
    "        Theoretical Semivariogram used for data interpolation. Use `get_semivariogram_model()` function to generate this model.\n",
    "        \n",
    "    target_col : str\n",
    "        \n",
    "        The column where the predicted value will be stored.\n",
    "        \n",
    "    error_col : str\n",
    "        \n",
    "        The column where the standard error will be stored.\n",
    "        \n",
    "    n_neighbors : int\n",
    "        \n",
    "        Number of neighbors to use for weighting. By default, all neighbors are used in the prediction (i.e. value: -1)\n",
    "        \n",
    "    \"\"\"\n",
    "    # Create empty column for interpolated values\n",
    "    gdf[target_col] = None\n",
    "\n",
    "    # Interpolate the values with the test dataset\n",
    "    for row in gdf.itertuples():\n",
    "        index = row.Index\n",
    "        point = np.array([row.geometry.centroid.x, row.geometry.centroid.y])\n",
    "\n",
    "        # Use Ordinary Kriging\n",
    "        result = model.ordinary_kriging(point, \n",
    "                                        number_of_neighbours=n_neighbors, \n",
    "                                        test_anomalies=False)\n",
    "\n",
    "        # The Kriging interpolation returns various things (as a tuple)\n",
    "        # Here, we unpack the values into their own variables.\n",
    "        # 'predicted' value for given location, standard 'error' for the estimate\n",
    "        # 'weights' that were applied for given point \n",
    "        predicted, error, estimated_mean, weights = result\n",
    "\n",
    "        # Assign the value to the result\n",
    "        gdf.at[index, target_col] = round(predicted, 2)\n",
    "        gdf.at[index, error_col] = error\n",
    "    return gdf\n",
    "\n",
    "\n",
    "\n",
    "def calculate_RMSE(gdf, validation_col=\"F3\", prediction_col=\"predicted_value\"):\n",
    "    \"\"\"\n",
    "    A helper function to calculate the Root Mean Square Error (RMSE) for predicted values. \n",
    "    The predictions are compared against the known values (validation).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame\n",
    "        \n",
    "        GeoDataFrame that should contain both the observed and predicted values.\n",
    "        \n",
    "    validation_col : str\n",
    "        \n",
    "        Column name for the observed values (i.e. the values that we know beforehand)\n",
    "        \n",
    "    prediction_col : str\n",
    "        \n",
    "        Column name for the predicted values (i.e. the values that were estimated using interpolation)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    return mean_squared_error(gdf[validation_col].values, gdf[prediction_col].values, squared=False)\n",
    "\n",
    "\n",
    "def get_pointset_for_interpolation(gdf, value_column):\n",
    "    \"\"\"\n",
    "    A helper function to extract a pointset (numpy arrays) for interpolation. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    \n",
    "    gdf: gpd.GeoDataFrame \n",
    "        \n",
    "        Input GeoDataFrame from which the pointset for interpolation are parsed. The x and y coordinates\n",
    "        are parsed from the geometry, and the z values are parsed from the selected column.\n",
    "        In case the input geometries are polygons, a centroid of the given geometry is returned.\n",
    "        \n",
    "    value_column : str\n",
    "        \n",
    "        The name of the column that contains the values (z) for the attribute of interest \n",
    "        (e.g. the amount of copper observed at given location). \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    # Extract x, y coordinates and the attribute (z) values from the gdf \n",
    "    x = gdf.geometry.centroid.x.values\n",
    "    y = gdf.geometry.centroid.y.values\n",
    "    z = gdf[value_column].values\n",
    "    return np.array([x, y, z]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06161edf-a281-45b7-bdd9-27883df7a54d",
   "metadata": {},
   "source": [
    "## Problem 1 - Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d087e923-751e-4754-b9b6-7792496546fe",
   "metadata": {},
   "source": [
    "### Task 1.1 - Divide the data into training and test datasets (4 points)\n",
    "\n",
    "**1. Read the data** file `data/copper_data.geojson` with geopandas into a variable called `data`(*the result will be a GeoDataFrame*).\n",
    "\n",
    "**2. Generate an interactive map** using the [`.explore()` function](https://geopandas.org/en/stable/docs/reference/api/geopandas.GeoDataFrame.explore.html) of geopandas, in which you should visualize the copper content in the column `\"F3\"`. You can use the classification scheme `quantiles` to better differentiate the patterns in the data (pass `scheme=\"quantiles\"` parameter into the `.explore()` function). See an example map below. \n",
    "\n",
    "**3. Divide the data into training and test datasets** by randomly picking 75 % of the data for training and 25 % for test:\n",
    "  - Store the training data into variable `train` and test data into variable `test`\n",
    "  - **Hint 1**: For random sampling the 75 % of the data into variable `train`, you can use the `.sample()` method of pandas (see [docs for help](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html))\n",
    "  - **Hint 2**: For selecting the remaining 25 % of the data, you should pick the rows the `data` which were **not** selected into the 75 % random sample. For doing this, you can take advantage of the index values in the `data` and `train`, and conduct an inverse selection using tilde (`~`) and `isin()` method. See [this StackOverflow answer](https://stackoverflow.com/a/44318806) for help.\n",
    "\n",
    "**4. Genererate an interactive map** which shows the training points with red color and the test points with blue color. See an example map below.\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "\n",
    "\n",
    "![Copper content](img/copper_content.png)\n",
    "*Example 1: The copper content visualized using quantiles classification (step 2 above).*\n",
    "\n",
    "![Training and test points](img/training_and_test_points.png)\n",
    "\n",
    "*Example 2: Randomly sampled points for training (75 %) and testing (25 %). Notice that the pattern you will see is likely different because they are picked randomly. (step 4 above)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e58dd-7617-4dc6-a576-cb6817b2d889",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ed0894ec58e4c078a43c33f152bf6e8",
     "grade": true,
     "grade_id": "cell-240332c7c9c4521f",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa520f4e-03f7-472f-b079-b0c95ac42563",
   "metadata": {},
   "source": [
    "### Task 1.2 - Generate a hexagon grid (2 points)\n",
    "\n",
    "The hexagon grid will be used for producing continuous interpolation surface in the problems 2 and 3.\n",
    "\n",
    "**1. Generate a hexagon grid** (as below) by using the `create_hexagon_grid()` function provided for you in the [Helper functions](#Helper-functions). **Store the result** into variable `grid`. You can use the default gridsize (i.e. 60). \n",
    "  - Hint: Read the function docstring in [Helper functions](#Helper-functions) to understand how it works.\n",
    "**2. Generate an interactive map** showing the grid using `.explore()` that should look something like below:\n",
    "\n",
    "![Hexagon grid](img/hexagon_grid.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cf09e0-e2c7-40ee-81e3-edf2df9e3973",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "21a6fec7e54f61c98543cab418e6db7c",
     "grade": true,
     "grade_id": "cell-07625de61096969e",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c647530c-efba-49b5-90a7-0ba5a598499f",
   "metadata": {},
   "source": [
    "## Problem 2 - Interpolation with IWD\n",
    "\n",
    "In this problem, the objective is to conduct interpolation using Inverse Distance Weighting algorithm, and test how changing the `power` influences the results and accuracy of the prediction. All the interpolations that we do in this exercise are all based on the [`pyinterpolate` -library](https://pyinterpolate.readthedocs.io/en/latest/).\n",
    "\n",
    "### Task 2.1 - Find out the best performing $\\beta$ in IDW (3 points)\n",
    "\n",
    "**1. Use the `interpolate_idw()` function to conduct IDW interpolation** which is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details):\n",
    "\n",
    "  - You should use the `train` dataset as the known data points\n",
    "  - Use the `test` dataset as the points which are unknown, i.e. the predictions will be done for those locations (and saved into this table)\n",
    "    - Notice that in reality, we do know the real copper values for the points in `test` but here we \"pretend\" that we don't know them, because in this way we can assess the accuracy of our prediction.\n",
    "  - The column `\"F3\"` in the `train` dataset contains the values (copper content) that we want to use in prediction\n",
    "  - Use **a power of 1** in the interpolation (i.e. $\\beta$)\n",
    "  - Specify that you want to store the result in a **column called `\"idw_power_1\"`**\n",
    "  - Store the result from this function into a variable `test` (i.e. the `test` dataset that was passed as one of the argument will be updated)\n",
    "    \n",
    "**2. Calculate the Root Mean Square Error (RMSE) for the prediction** using the `calculate_RMSE()` -function that is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it):\n",
    "\n",
    "  - Pass the result from step 1 (i.e. should be the variable `test`)\n",
    "  - Use the `\"F3\"` column for the validation\n",
    "  - Use the column `\"idw_power_1\"` as the `prediction_col`\n",
    "  - Store the result into a new variable called `idw_p1`\n",
    "  - Print out the RMSE for the interpolation showing how well the predicted values compare against the known values. The lower the RMSE, the better the prediction.\n",
    "  \n",
    "**3. Repeat the steps 1 and 2** and test how **changing the power** ($\\beta$) influences the accuracy of the result. \n",
    "\n",
    "  - Repeat the analysis with **power values: 2,3 and 4**. Store the results into variables `idw_p2`, `idw_p3` and `idw_p4` accordingly. \n",
    "  - Which $\\beta$ produces the **best** result?\n",
    "  - Which $\\beta$ produces the **worst** result? Why do you think it performs poorly?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5387426-532e-4534-923a-39659554e21c",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "207bb1f162e9a82a5ff60b1f5ed8cb93",
     "grade": true,
     "grade_id": "cell-c944114547702dbd",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce540114-8abb-480f-b24f-f694dc7c9ba7",
   "metadata": {},
   "source": [
    "### Task 2.2 - Create a map with IDW predictions (2 points)\n",
    "\n",
    "Use the power that had the best performance in the previous step, and create a map using the hexagon grid created in Task 1.2.\n",
    "\n",
    "**1. Use the `interpolate_idw()` function** and:\n",
    "  \n",
    "  - use the `train` dataset as the known locations\n",
    "  - use the hexagon `grid` as the unknown locations to which we will predict values using IDW\n",
    "  - use the $\\beta$ that performed best\n",
    "  - use again the `\"F3\"` as the column for known values\n",
    "  - specify that the results should be stored into column `idw_power_X`, where you should replace the `X` with the power that you choose to use\n",
    "  - store the result from the function into variable `grid` (i.e. we will update the hexagon grid created earlier)\n",
    "  \n",
    "**2. Create a map with the hexagon grid** where the color of the cell should be the interpolated value from step 1. Use a colormap `\"Reds\"` that you can define with parameter `cmap`. If you want, you can add the `test` points on top of this hexagon grid which allows you to easily evaluate visually how well the predictions have worked at different locations. As a result, you should get something like the map below.\n",
    "\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "  \n",
    "![IDW interpolation](img/idw_interpolation.png)\n",
    "\n",
    "*Example map how the result from IDW interpolation should look like (approximately).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676685cf-ac82-4bcd-aef1-44820ad2be1f",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cab275775d7aaa424f974faf509f426d",
     "grade": true,
     "grade_id": "cell-b59a65e4867c3d90",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f890e039-5ae3-42af-abf3-47b00b942d68",
   "metadata": {},
   "source": [
    "## Problem 3 - Interpolation with Kriging\n",
    "\n",
    "In this problem, we will continue working with the same data and use **Ordinary Kriging** to predict the values (copper content) for unknown locations.\n",
    "To do this, we will:\n",
    "\n",
    "1. Check whether the data is normally distributed. If not:\n",
    "2. Transform the data by taking a log of the values, which helps to \"normalize\" the data\n",
    "3. Find the optimal semivariogram model for the data, and test how changing the parameters influence the model fit\n",
    "4. Conduct Ordinary Kriging for the `test` dataset with the selected model and validate the result by calculating the RMSE \n",
    "5. Visualize the interpolation result as well as the standard errors that shows where the prediction works well and where not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a382eb53-4200-4e09-98df-a5516a855ff7",
   "metadata": {},
   "source": [
    "### Task 3.1 - Testing the assumptions (2 points)\n",
    "\n",
    "**1. Is the data normally distributed?**:\n",
    "\n",
    "  - Plot a histogram of the `\"F3\"` values using the `.plot.hist()` function of pandas ([see docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.hist.html) for help). Use 30 bins which helps showing the differences at different value classes. \n",
    "  - To make a better visual assessment of the normality, also make a [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot) that helps to estimate whether your data points follow normal distribution. For this, you can use the [`qqplot()` -function](https://www.statsmodels.org/stable/generated/statsmodels.graphics.gofplots.ProbPlot.qqplot.html) from the `statsmodels` library. See [this StackOverflow answer](https://stackoverflow.com/a/50677734) for reference.\n",
    "  - Based on these plots, do you think the data follows normal distribution? Justify your answer.\n",
    "    - Your answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d75298c-c077-4b53-a81c-c9bfdca76b57",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "66c7fd5d39d45b2c8b867edec8c9936f",
     "grade": true,
     "grade_id": "cell-ff055018306b52d5",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff52877-da25-43a7-9e04-2ae7965c08b7",
   "metadata": {},
   "source": [
    "**2. Log transform the data** (if it does not follow normal distribution):\n",
    "\n",
    "In case your data was not normally distributed, it needs to be transformed in such a way that it follows normal distribution. There are a few different approaches for doing this, and one of the most common approaches is to log-transform your data which helps to make the data as \"normal\" as possible. (*However, note that log-transform is not always recommended e.g. [with count data having zeros](https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.2041-210X.2010.00021.x)*).\n",
    "\n",
    "  - You can log-transform you data by using the  [`np.log()` -function](https://numpy.org/doc/stable/reference/generated/numpy.log.html) and applying it for the `\"F3\"` column. \n",
    "  - Store the data into a new column called `\"log_F3\"`\n",
    "  - Test with Q-Q plot and histogram whether the data now follows better the normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79deb591-d977-4d49-85bd-7cf4cf9c228e",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5f3100ec7f0f7b121e22e603ea7c7c4c",
     "grade": true,
     "grade_id": "cell-1697942f217a68dd",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Add your code here IF NEEDED\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f95a25-2424-4038-82b1-94e316bb303a",
   "metadata": {},
   "source": [
    "## Task 3.2 - Find optimal semivariogram model (2 points)\n",
    "\n",
    "As a good background information for this task, I recommend to check [here a good explanation](https://pyinterpolate.readthedocs.io/en/latest/algorithms_documentation/Automatic%20Fitting%20of%20the%20Semivariogram%20Model.html) of how the optimal model can be found using the `pyinterpolate` library. Also it is **highly recommended** that you check the [slides for geostatistics](https://spatial-analytics.readthedocs.io/en/latest/lessons/L2/geostatistics-kriging.html) and [watch the lesson video](https://spatial-analytics.readthedocs.io/en/latest/lessons/L2/overview.html#lesson-videos), in case you want to revise what was said about the things relating to the questions below.\n",
    "\n",
    "**1. Investigate your data** using the interactive maps done in earlier tasks, and update the variables (also just test how changing the values influences the result!):\n",
    "    \n",
    "  - Think what would be a good **search radius** for the data? The search radius defines the distance between lags within each points are included in the calculations. \n",
    "    - **Modify the value for the `search_radius` variable below** (should be in meters). **Describe why/how did you end up selecting the given value?**\n",
    "    - Your answer:\n",
    "  \n",
    "  - Investigate the interactive maps, and think what would be the maximum possible distance between two points in the dataset? \n",
    "    - **Update the `max_range` variable** below based on your findings (should be in meters).\n",
    "    - Hint: take advantage of the scalebar on the bottom left corner of the interactive maps. It updates when you zoom in/out. \n",
    "  \n",
    "  - Think, whether the influence of the nearest points for given location should be **weighted or not**? \n",
    "  \n",
    "    - **Update the `weighted` variable** below to `False` or `True` based on your thinking. Describe why did end up in your decision:\n",
    "    - Your answer:\n",
    "\n",
    "**2. Get the optimal semivariogram model** based on your parameters defined in step 1:\n",
    "\n",
    "   - **Use the `get_semivariogram_model()`** provided for you in the [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it).\n",
    "   - Use the `train` dataset again as the input gdf\n",
    "   - Use the `\"F3\"` or `\"log_F3\"` attribute as value column, depending on whether you transformed the data or not\n",
    "   - Pass the `search_radius`, `max_range`, `weighted` and `number_of_ranges` variables for the function (check the function docstring to know how the parameters are called)\n",
    "   - Save the resulting model that is returned by the `get_semivariogram_model()` into a variable `model`\n",
    "   \n",
    "**This model is used for determining the weights for the interpolation**, and in practice used for doing the interpolation (see next step).    \n",
    "\n",
    "As a result of this, you should end up having something like following (it can look different due to different parameter values, and it's okay):\n",
    "![Semivariogram model](img/semivariogram_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f27366-2af7-4428-bdc8-9138fffa98ed",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa48b7d54be2389f95db131518b94262",
     "grade": true,
     "grade_id": "cell-863e5d8441c92912",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Search radius (in meters in this case because our points are in metric system)\n",
    "search_radius = 0\n",
    "\n",
    "# The maximum distance between points\n",
    "max_range = 0\n",
    "\n",
    "# Weighting according to lags (True/False)\n",
    "weighted = \"ADD TRUE OR FALSE\"\n",
    "\n",
    "# Algorithm divides the study extent into n number of ranges and tests \n",
    "# the theoretical model output against the experimental variogram for each range\n",
    "# Note: This can be as it is.\n",
    "number_of_ranges = 20\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b878cd-5394-401c-84eb-7ef94350f6e2",
   "metadata": {},
   "source": [
    "## Task 3.3 - Interpolate with Ordinary Kriging (2 points)\n",
    "\n",
    "After you have defined and created a semivariogram model in the previous step, we can do the interpolation.\n",
    "\n",
    "**1. Use the `interpolate_ordinary_kriging()` -function** provided for you in the [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it).\n",
    "\n",
    "  - Use the `test` dataset as the gdf for the function (i.e. the predictions will be done again for those points)\n",
    "  - Use the semivariogram `model` created in the previous step representing the Theoretical Semivariogram used for data interpolation.\n",
    "  - Give an intuitive name for the column where the predicted values will be stored, such as `\"predicted_krige\"` or `\"predicted_krige_log\"`\n",
    "  - Give an intuitive name for the column where the standard error of the predictions will be stored, such as `\"std_error\"` or `\"std_error_log\"`\n",
    "  - Store the result that is returned from the `interpolate_ordinary_kriging()` into the variable `test` (i.e. we will again update that GeoDataFrame with new columns)\n",
    "  \n",
    "**2.** In case you have transformed the data, **back-transform the interpolation predictions and standard errors** by calculating the exponential of all predicted and standard error values:\n",
    "\n",
    "  - Store the back-transformed values into `test` dataset with intuitive column names, such as `predicted_krige` and `std_error`\n",
    "  - You can calculate the exponential by using the [`np.exp()` -function](https://numpy.org/doc/stable/reference/generated/numpy.exp.html)\n",
    "  - After this step, your predicted values are on the same scale as the original values in `\"F3\"` column.\n",
    "  \n",
    "**3. What is the RMSE for the Kriging predictions?**\n",
    "\n",
    "  - Calculate the Root Mean Square Error (RMSE) for the prediction** using the `calculate_RMSE()` -function that is provided for you in [Helper functions](#Helper-functions) (read the documentation/docstring of the function for further details how to use it):\n",
    "\n",
    "  - Pass the `test` GeoDataFrame as the gdf\n",
    "  - Use the `\"F3\"` column for the validation\n",
    "  - Use the `\"predicted_krige\"` column as the `prediction_col` (i.e. the column having the values on the same scale as `\"F3\"`)\n",
    "  - Store the result into a new variable called `krige_rmse`\n",
    "  - Print out the RMSE for the interpolation showing how well the predicted values compare against the known values. The lower the RMSE, the better the prediction. \n",
    "  - Question: Did the Kriging interpolation perform better than IDW? Justify your answer with a short description.\n",
    "    - Your answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944c138-293f-4e3f-9fc2-e637def41faf",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "121a475883a8feb03dae126d2ac200fb",
     "grade": true,
     "grade_id": "cell-7abb2a440004a023",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee482fd4-dd36-4704-a230-2e6d3b0adffa",
   "metadata": {},
   "source": [
    "### Task 3.4 - Create a map with Ordinary Kriging predictions (2 points)\n",
    "\n",
    "**1. Use the `interpolate_ordinary_kriging()` function** and:\n",
    "  \n",
    "  - use the hexagon `grid` as the unknown locations to which we will predict values using IDW\n",
    "  - Use the semivariogram `model` created in the previous step representing the Theoretical Semivariogram used for data interpolation.\n",
    "  - Give an intuitive name for the column where the predicted values will be stored, such as `\"predicted_krige\"` or `\"predicted_krige_log\"`\n",
    "  - Give an intuitive name for the column where the standard error of the predictions will be stored, such as `\"std_error\"` or `\"std_error_log\"`\n",
    "  - Store the result from the function into variable `grid` (i.e. we will update the hexagon grid created earlier)\n",
    "  - If needed, back-transform the values again in a similar manner as in Task 3.3.\n",
    "  \n",
    "**2. Create a map with the predicted values using the hexagon grid**, where the color of the cell should be the predicted values from step 1. Use a colormap `\"Reds\"` that you can define with parameter `cmap`. If you want, you can again add the `test` points on top of this hexagon grid which allows you to easily evaluate visually how well the predictions have worked at different locations. As a result, you should get something like the map below.\n",
    "\n",
    "  - **Hint:** Adding multiple layers with `.explore()` is possible as shown in the [last example on this page](https://geopandas.org/en/stable/docs/user_guide/interactive_mapping.html#Interactive-mapping). The examples on the page also shows how you can adjust different visualization parameters, such as color of the points.\n",
    "  \n",
    "  - **Question**: Do you think the result differs significantly from the IDW interpolation? Give a short explanation.\n",
    "\n",
    "**3. Create a map with the standard error values using the hexagon grid**, where the color of the cell should be the standard error values. Use a colormap `\"Blues\"` that you can define with parameter `cmap`. As a result, you should get something like the map below.\n",
    "\n",
    "  - **Question 1 \\***: Do you see any evident patterns in the standard errors? Give a short explanation. \n",
    "    - Your answer:\n",
    "  - **Question 2:** What is the purpose of Std Error map? Why is it useful?\n",
    "    - Your answer:\n",
    "  \n",
    "\\*_Further info related to Question 1: Clusters or trends in the standard errors would indicate that it would be good to try to remove the trends and consider anisotropy in the interpolation process. We won't be considering these things in this exercise, but if you are interested to learn more and take into account anisotropy there is another Python package called [PyKrige](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/index.html) that [can handle anisotropy](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/generated/pykrige.ok.OrdinaryKriging.html). The library also provides [Universal Kriging](https://geostat-framework.readthedocs.io/projects/pykrige/en/stable/generated/pykrige.uk.UniversalKriging.html) method allowing to use drift terms._ \n",
    "\n",
    "![Ordinary Kriging predictions example](img/ordinary_kriging_predictions.png)\n",
    "\n",
    "___Example 1__ of the resulting map for the Ordinary Kriging predicted values._\n",
    "\n",
    "\n",
    "![Ordinary Kriging std errors example](img/ordinary_kriging_std_errors.png)\n",
    "\n",
    "___Example 2__ of the resulting map for the Ordinary Kriging standard error values._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9b63d5-14b0-483d-be90-97ead8635bbb",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ff03b766da905c5526bd9631f2c4632e",
     "grade": true,
     "grade_id": "cell-e4bdbe9f8c8b0c9b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6fe782-66f3-4c1e-ab96-40c1715a7e71",
   "metadata": {},
   "source": [
    "## Problem 4 - How long did it take? Optional feedback (1 point)\n",
    "\n",
    "To help developing the exercises, and understanding the time that it took for you to finish the Exercise, please provide an estimate of how many hours you spent for doing this exercise?\n",
    "\n",
    " - I spent approximately this many hours: **X hours**\n",
    " \n",
    "In addition, if you would like to give any feedback about the exercise (optional), please provide it below:\n",
    "\n",
    " - My feedback:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
